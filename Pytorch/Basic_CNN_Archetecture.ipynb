{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.Tensor(5,3)\n",
    "x = t.Tensor([[1,2],[3,4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7537, 0.2159, 0.5056],\n",
      "        [0.4562, 0.9566, 0.8105],\n",
      "        [0.8472, 0.6623, 0.3249],\n",
      "        [0.0277, 0.7713, 0.9531],\n",
      "        [0.2288, 0.3497, 0.0284]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = t.rand(5, 3)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = t.ones(2, 2, requires_grad=True) # We want to let Tensor to use 'autograd'\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x000002ADA2F23E08>\n"
     ]
    }
   ],
   "source": [
    "y = x.sum()\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<SumBackward0>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "y.backward() # Backward Propagation\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # nn.Module.__init__(self)\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # '1'表示輸入圖片為單通道，'6'表示輸出通道數，'5'表示卷積核為5 * 5\n",
    "        # Convolutional Layer (卷積層)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Affine Layer (仿射層) / Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution -> Activation -> Pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # reshape '-1'表示自適應\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 只要在nn.Module的子類中定義了forward函數，backward函數就會自動被實現(利用`autograd`)。在`forward` 函數中可使用任何tensor支持的函數，還可以使用if、for循環、print、log等Python語法，寫法和標準的Python寫法一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 網絡的可學習參數通過`net.parameters()`返回，`net.named_parameters`可同時返回可學習的參數及名稱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight : torch.Size([6, 1, 5, 5])\n",
      "conv1.bias : torch.Size([6])\n",
      "conv2.weight : torch.Size([16, 6, 5, 5])\n",
      "conv2.bias : torch.Size([16])\n",
      "fc1.weight : torch.Size([120, 400])\n",
      "fc1.bias : torch.Size([120])\n",
      "fc2.weight : torch.Size([84, 120])\n",
      "fc2.bias : torch.Size([84])\n",
      "fc3.weight : torch.Size([10, 84])\n",
      "fc3.bias : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in net.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 `input.unsqueeze(0)`将batch_size设为１。例如 `nn.Conv2d` 输入必须是4维的，形如$nSamples \\times nChannels \\times Height \\times Width$。可将nSample设为1，即$1 \\times nChannels \\times Height \\times Width$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "tensor([[[[ 0.3459, -2.0160,  0.1444,  ...,  0.2156, -0.6544, -1.8561],\n",
      "          [-0.1484,  0.4291,  0.9794,  ..., -0.4742, -0.1758, -2.0740],\n",
      "          [-1.2784,  1.5764,  0.2369,  ..., -0.4243, -1.1019, -0.8197],\n",
      "          ...,\n",
      "          [-0.2057,  1.0419, -1.4551,  ...,  0.4286, -0.9357, -0.4304],\n",
      "          [ 0.6962, -1.0762,  1.3391,  ...,  1.3006, -1.3452,  1.9534],\n",
      "          [-0.8572, -0.8661,  1.8086,  ..., -0.9926, -1.0486, -0.8383]]]])\n"
     ]
    }
   ],
   "source": [
    "Input = t.randn(1, 1, 32, 32)\n",
    "print(Input.shape)\n",
    "print(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([[ 0.0890,  0.0012, -0.0146,  0.0189,  0.0161, -0.1236, -0.0203,  0.0569,\n",
      "         -0.0710, -0.0031]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Out = net(Input)\n",
    "print(Out.shape)\n",
    "print(Out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "\n",
    "nn實現了神經網絡中大多數的損失函數，例如nn.MSELoss用來計算均方誤差，nn.CrossEntropyLoss用來計算交叉熵損失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.6721, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ouput = net(Input)\n",
    "target = t.arange(0,10).view(1,10).float() #just like reshape\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(Ouput, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果對loss進行反向傳播溯源(使用`gradfn`屬性)，可看到它的計算圖如下：\n",
    "```\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n",
    "      -> view -> linear -> relu -> linear -> relu -> linear \n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "```\n",
    "當調用`loss.backward()`時，該圖會動態生成並自動微分，也即會自動計算圖中參數(Parameter)的導數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反向傳播之前 conv1.bias的梯度\n",
      "None\n",
      "反向傳播之後 conv1.bias的梯度\n",
      "tensor([-0.0300,  0.1192,  0.0406, -0.1068, -0.0778,  0.0823])\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "net.zero_grad() # 把net中所有可學習參數的梯度清0\n",
    "print('反向傳播之前 conv1.bias的梯度')\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print('反向傳播之後 conv1.bias的梯度')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0958, -0.0821, -0.1688, -0.2241, -0.0565],\n",
      "          [ 0.0513, -0.1301, -0.0328,  0.0529, -0.1633],\n",
      "          [ 0.0319, -0.0489,  0.0105, -0.1292, -0.0387],\n",
      "          [ 0.0142,  0.0369,  0.0479, -0.1536, -0.0206],\n",
      "          [ 0.0713, -0.0626, -0.1043, -0.1084, -0.0190]]],\n",
      "\n",
      "\n",
      "        [[[-0.0252,  0.1546, -0.0709,  0.0597,  0.1519],\n",
      "          [-0.0699,  0.0589,  0.0693, -0.0234,  0.1426],\n",
      "          [ 0.0510,  0.0026, -0.1293, -0.0176,  0.0785],\n",
      "          [-0.0019,  0.1655,  0.0004, -0.1150,  0.0089],\n",
      "          [ 0.0150,  0.0066,  0.0283,  0.0059, -0.0356]]],\n",
      "\n",
      "\n",
      "        [[[-0.0178, -0.0336, -0.0654,  0.0027, -0.0717],\n",
      "          [-0.0642,  0.1092, -0.1425, -0.1365, -0.0334],\n",
      "          [-0.0085,  0.0558,  0.0266, -0.1151, -0.0742],\n",
      "          [ 0.0342, -0.1131, -0.0182, -0.0647, -0.0768],\n",
      "          [-0.0952, -0.0263, -0.1068,  0.0266, -0.0138]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1121,  0.0271, -0.0050,  0.0367,  0.1382],\n",
      "          [ 0.0828,  0.0312, -0.0333,  0.0853,  0.0702],\n",
      "          [-0.0440, -0.0760,  0.0235,  0.0825,  0.0066],\n",
      "          [ 0.0301,  0.0768, -0.1085,  0.0827,  0.0060],\n",
      "          [-0.0513, -0.0374,  0.0425,  0.0601,  0.0097]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0076,  0.0492,  0.0566,  0.1570,  0.0306],\n",
      "          [-0.0629, -0.1298, -0.0533,  0.0277,  0.0274],\n",
      "          [ 0.0098, -0.0029,  0.0116,  0.0187,  0.0545],\n",
      "          [ 0.0825,  0.0139, -0.0478, -0.0036, -0.0114],\n",
      "          [-0.0642, -0.0370,  0.0722,  0.0504, -0.0153]]],\n",
      "\n",
      "\n",
      "        [[[-0.0445, -0.1264, -0.0967,  0.0665, -0.0378],\n",
      "          [ 0.0098, -0.0360, -0.0372,  0.1313, -0.0067],\n",
      "          [-0.0163,  0.0265, -0.1353, -0.0478,  0.0345],\n",
      "          [ 0.0634, -0.0019, -0.0436,  0.0480,  0.0292],\n",
      "          [-0.0648,  0.0203, -0.1608,  0.0185, -0.0450]]]])\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向傳播計算完所有參數的梯度後，還需要使用優化方法來更新網絡的權重和參數，例如隨機梯度下降法(SGD)的更新策略如下：\n",
    "```\n",
    "weight = weight - learning_rate * gradient\n",
    "```\n",
    "\n",
    "手動實現如下：\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)# inplace 減法\n",
    "```\n",
    "\n",
    "`torch.optim`中實現了深度學習中絕大多數的優化方法，例如RMSProp、Adam、SGD等，更便於使用，因此大多數時候並不需要手動寫上述代碼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#新建一個優化器，指定要調整的參數和學習率\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "# 在訓練過程中\n",
    "# 先梯度清零(與net.zero_grad()效果一樣)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 計算損失\n",
    "Output = net(Input)\n",
    "loss = criterion(Output, target)\n",
    "\n",
    "#反向傳播\n",
    "loss.backward()\n",
    "\n",
    "#更新參數\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
