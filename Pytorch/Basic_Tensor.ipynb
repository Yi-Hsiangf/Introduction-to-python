{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From https://github.com/chenyuntc/pytorch-book.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch  as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從接口的角度來講，對tensor的操作可分為兩類：\n",
    "\n",
    "1. `torch.function`，如`torch.save`等。\n",
    "2. 另一類是`tensor.function`，如`tensor.view`等。\n",
    "\n",
    "為方便使用，對tensor的大部分操作同時支持這兩類接口，在本書中不做具體區分，如`torch.sum (torch.sum(a, b))`與`tensor.sum (a. sum(b))`功能等價。\n",
    "\n",
    "而從存儲的角度來講，對tensor的操作又可分為兩類：\n",
    "\n",
    "1. 不會修改自身的數據，如 `a.add(b)`， 加法的結果會返回一個新的tensor。\n",
    "2. 會修改自身的數據，如 `a.add_(b)`， 加法的結果仍存儲在a中，a被修改了。\n",
    "\n",
    "函數名以`_`結尾的都是inplace方式, 即會修改調用者自己的數據，在實際應用中需加以區分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 創建Tensor\n",
    "\n",
    "在PyTorch中新建tensor的方法有很多，具體如表3-1所示。\n",
    "\n",
    "表3-1: 常見新建tensor的方法\n",
    "\n",
    "|函數|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(\\*sizes)|基礎構造函數|\n",
    "|tensor(data,)|類似np.array的構造函數|\n",
    "|ones(\\*sizes)|全1Tensor|\n",
    "|zeros(\\*sizes)|全0Tensor|\n",
    "|eye(\\*sizes)|對角線為1，其他為0|\n",
    "|arange(s,e,step|從s到e，步長為step|\n",
    "|linspace(s,e,steps)|從s到e，均勻切分成steps份|\n",
    "|rand/randn(\\*sizes)|均勻/標準分佈|\n",
    "|normal(mean,std)/uniform(from,to)|正態分佈/均勻分佈|\n",
    "|randperm(m)|隨機排列|\n",
    "\n",
    "這些創建方法都可以在創建的時候指定數據類型dtype和存放device(cpu/gpu).\n",
    "\n",
    "\n",
    "其中使用`Tensor`函數新建tensor是最複雜多變的方式，它既可以接收一個list，並根據list的數據新建tensor，也能根據指定的形狀新建tensor，還能傳入其他的tensor，下面舉幾個例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.3867e+20, 9.2358e-01, 1.8061e+28],\n",
       "        [4.4378e+27, 5.5388e-14, 1.6109e-19]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use List\n",
    "b = t.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(b.size()) # b.shape\n",
    "print(b.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8370e+25, 1.4603e-19, 1.2102e+25],\n",
       "        [1.6217e-19, 3.0881e+29, 1.2102e+25]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.Tensor(b.size())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1,6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9650, 0.9326, 0.0419],\n",
       "        [0.1856, 0.9682, 0.4638]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rand(2, 3, device=t.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1, 2, 5, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(3, 3, dtype=t.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar: tensor(3.1416), shape of sclar: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "scalar = t.tensor(3.14159) \n",
    "print('scalar: %s, shape of sclar: %s' %(scalar, scalar.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: tensor([5, 3]), shape of vector: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "vector = t.tensor([5,3])\n",
    "print('vector: %s, shape of vector: %s' %(vector, vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1000, 1.2000],\n",
       "         [2.2000, 3.1000],\n",
       "         [4.9000, 5.2000]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = t.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
    "matrix,matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1111, 0.2222, 0.3333]], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tensor([[0.11111, 0.222222, 0.3333333]], dtype=t.float64, device=t.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_tensor = t.tensor([])\n",
    "empty_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通過`tensor.view`方法可以調整tensor的形狀，但必須保證調整前後元素總數一致。 `view`不會修改自身的數據，返回的新tensor與源tensor共享內存，也即更改其中的一個，另外一個也會跟著改變。在實際應用中可能經常需要添加或減少某一維度，這時候`squeeze`和`unsqueeze`兩個函數就派上用場了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(-1, 3) # -1 => it will calculate the dimension itself\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "b.unsqueeze(1)\n",
    "print(b[:, None].shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(-2) # -2表示倒數第二個維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0, 1, 2],\n",
      "           [3, 4, 5]]]]])\n",
      "tensor([[[[0, 1, 2],\n",
      "          [3, 4, 5]]]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "print(c)\n",
    "print(c.squeeze(0)) # 壓縮第0維的 \"1\"\n",
    "print(c.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0313, -0.2236,  0.6810, -1.1241],\n",
       "        [-0.9615,  2.2381, -0.9188,  0.7759],\n",
       "        [-1.2850,  0.1560, -0.1778,  1.6034]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0313, -0.2236,  0.6810, -1.1241])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0313, -0.2236,  0.6810, -1.1241])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0313, -0.9615, -1.2850])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6810)\n",
      "tensor(-1.1241)\n"
     ]
    }
   ],
   "source": [
    "print(a[0][2])\n",
    "print(a[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# None類似於np.newaxis, 為a新增了一個軸\n",
    "# 等價於a.view(1, a.shape[0], a.shape[1])\n",
    "a[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4])\n",
      "torch.Size([3, 1, 4, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(a[:, None, :].shape)\n",
    "print(a[:,None,:,None,None].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0313, -0.2236,  0.6810, -1.1241],\n",
       "        [-0.9615,  2.2381, -0.9188,  0.7759],\n",
       "        [-1.2850,  0.1560, -0.1778,  1.6034]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它常用的選擇函數如表3-2所示。\n",
    "\n",
    "\n",
    "函數|功能|\n",
    ":---:|:---:|\n",
    "index_select(input, dim, index)|在指定維度dim上選取，比如選取某些行、某些列\n",
    "masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor進行選取\n",
    "non_zero(input)|非0元素的下標\n",
    "gather(input, dim, index)|根據index，在dim維度上選取數據，輸出的size與index一樣\n",
    "\n",
    "\n",
    "`gather`是一個比較複雜的操作，對一個2維tensor，輸出的每個元素如下：\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j] # dim=0\n",
    "out[i][j] = input[i][index[i][j]] # dim=1\n",
    "```\n",
    "三維tensor的`gather`操作同理，下面舉幾個例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = t.LongTensor([[0,1,2,3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]])\n",
      "tensor([[ 3],\n",
      "        [ 6],\n",
      "        [ 9],\n",
      "        [12]])\n"
     ]
    }
   ],
   "source": [
    "index = t.LongTensor([[3,2,1,0]]).t()\n",
    "print(index)\n",
    "print(a.gather(1, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逐元素操作\n",
    "\n",
    "這部分操作會對tensor的每一個元素(point-wise，又名element-wise)進行操作，此類操作的輸入與輸出形狀一致。常用的操作如表3-4所示。\n",
    "\n",
    "表3-4: 常見的逐元素操作\n",
    "\n",
    "|函數|功能|\n",
    "|:--:|:--:|\n",
    "|abs/sqrt/div/exp/fmod/log/pow..|絕對值/平方根/除法/指數/求餘/求冪..|\n",
    "|cos/sin/asin/atan2/cosh..|相關三角函數|\n",
    "|ceil/round/floor/trunc| 上取整/四捨五入/下取整/只保留整數部分|\n",
    "|clamp(input, min, max)|超過min和max部分截斷|\n",
    "|sigmod/tanh..|激活函數\n",
    "\n",
    "對於很多操作，例如div、mul、pow、fmod等，PyTorch都實現了運算符重載，所以可以直接使用運算符。如`a ** 2` 等價於`torch.pow(a,2)`, `a * 2`等價於`torch.mul(a,2)`。\n",
    "\n",
    "其中`clamp(x, min, max)`的輸出滿足以下公式：\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "min, & \\text{if } x_i \\lt min \\\\\n",
    "x_i, & \\text{if } min \\le x_i \\le max \\\\\n",
    "max, & \\text{if } x_i \\gt max\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "`clamp`常用在某些需要比較大小的地方，如取一個tensor的每個元素與另一個數的較大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[ 1.0000,  0.5403, -0.4161],\n",
      "        [-0.9900, -0.6536,  0.2837]])\n"
     ]
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3).float()\n",
    "print(a)\n",
    "print(t.cos(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [0., 1., 2.]])\n",
      "tensor([[ 0.,  1.,  4.],\n",
      "        [ 9., 16., 25.]])\n"
     ]
    }
   ],
   "source": [
    "print(a % 3)\n",
    "print(a ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 歸併操作\n",
    "此類操作會使輸出形狀小於輸入形狀，並可以沿著某一維度進行指定操作。如加法`sum`，既可以計算整個tensor的和，也可以計算tensor中每一行或每一列的和。常用的歸併操作如表3-5所示。\n",
    "\n",
    "表3-5: 常用歸併操作\n",
    "\n",
    "|函數|功能|\n",
    "|:---:|:---:|\n",
    "|mean/sum/median/mode|均值/和/中位數/眾數|\n",
    "|norm/dist|範數/距離|\n",
    "|std/var|標準差/方差|\n",
    "|cumsum/cumprod|累加/累乘|\n",
    "\n",
    "以上大多數函數都有一個參數**`dim`**，用來指定這些操作是在哪個維度上執行的。關於dim(對應於Numpy中的axis)的解釋眾說紛紜，這裡提供一個簡單的記憶方式：\n",
    "\n",
    "假設輸入的形狀是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，輸出的形狀就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，輸出的形狀就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，輸出的形狀就是(m, n, 1)或者(m, n)\n",
    "\n",
    "size中是否有\"1\"，取決於參數`keepdim`，`keepdim=True`會保留維度`1`。注意，以上只是經驗總結，並非所有函數都符合這種形狀變化方式，如`cumsum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "b.sum(dim = 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "b.sum(dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
